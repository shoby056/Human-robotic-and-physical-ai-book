"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[804],{3864:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>c});var o=i(4848),l=i(8453);const t={title:"Module 4 - Vision-Language-Action (VLA)",sidebar_label:"Module 4: Vision-Language-Action (VLA)"},a=void 0,s={id:"modules/module4-vla",title:"Module 4 - Vision-Language-Action (VLA)",description:"---",source:"@site/docs/modules/module4-vla.md",sourceDirName:"modules",slug:"/modules/module4-vla",permalink:"/docs/modules/module4-vla",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module4-vla.md",tags:[],version:"current",frontMatter:{title:"Module 4 - Vision-Language-Action (VLA)",sidebar_label:"Module 4: Vision-Language-Action (VLA)"}},r={},c=[{value:"title: Module 4 - Vision-Language-Action (VLA)\r\nsidebar_label: Module 4: Vision-Language-Action (VLA)",id:"title-module-4---vision-language-action-vlasidebar_label-module-4-vision-language-action-vla",level:2},{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function d(e){const n={h1:"h1",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"title-module-4---vision-language-action-vlasidebar_label-module-4-vision-language-action-vla",children:"title: Module 4 - Vision-Language-Action (VLA)\r\nsidebar_label: Module 4: Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This module explains Vision-Language-Action models for multimodal robotics applications and human-robot interaction."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand Vision-Language-Action model architectures"}),"\n",(0,o.jsx)(n.li,{children:"Implement multimodal perception systems"}),"\n",(0,o.jsx)(n.li,{children:"Create human-robot interaction using VLA models"}),"\n",(0,o.jsx)(n.li,{children:"Apply VLA models to robotic manipulation tasks"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Vision-Language-Action Fundamentals"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal Neural Networks"}),"\n",(0,o.jsx)(n.li,{children:"Cross-Modal Attention Mechanisms"}),"\n",(0,o.jsx)(n.li,{children:"Embodied AI Concepts"}),"\n",(0,o.jsx)(n.li,{children:"Human-Robot Interaction"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation with VLA Models"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implementing a VLA-based instruction follower"}),"\n",(0,o.jsx)(n.li,{children:"Creating multimodal perception pipeline"}),"\n",(0,o.jsx)(n.li,{children:"Building human-robot interaction system"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"VLA models represent the next generation of AI systems that combine perception, language understanding, and action planning."})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(6540);const l={},t=o.createContext(l);function a(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:a(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);